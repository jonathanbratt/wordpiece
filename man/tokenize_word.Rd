% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tokenization.R
\name{tokenize_word}
\alias{tokenize_word}
\title{Tokenize a single "word" (no whitespace).}
\usage{
tokenize_word(word, vocab, unk_token = "[UNK]", max_chars = 100)
}
\arguments{
\item{word}{Word to tokenize.}

\item{vocab}{Character vector containing vocabulary words}

\item{unk_token}{Token to represent unknown words.}

\item{max_chars}{Maximum length of word recognized.}
}
\value{
Input word as a list of tokens.
}
\description{
In BERT: tokenization.py,
this code is inside the tokenize method for WordpieceTokenizer objects.
I've moved it into its own function for clarity.
Punctuation should already have been removed from the word.
}
\examples{
tokenize_word("unknown", vocab = c("un" = 0, "##known" = 1))
tokenize_word("known", vocab = c("un" = 0, "##known" = 1))
}
